{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global Variables\n",
    "\n",
    "MODEL_NAME = 'flatband_Hyperedge_test'\n",
    "SUPERCELL_SIZE = 3 # Size of supercell in (NxN) unit cells\n",
    "SUPERCELL_SIZE_1 = 3\n",
    "SUPERCELL_SIZE_2 = 4\n",
    "#CUT_OFF_DISTANCE = 5  # Distance in angstroms below which nodes are connected with edges\n",
    "#MASKING_PERCENTAGE = 0.1 # Percentage of nodes features and edge attributes that are masked\n",
    "BATCH_SIZE = 64 # Number of materials ran through the network at a time\n",
    "NUM_EPOCHS = 1000 # How many times each trainer runs through the training data\n",
    "LEARNING_RATE = 1e-3 # Adjusts how sensitive the network is when changing the weights\n",
    "EMBEDDING_DIMENSION = 128 # How many dimensions the final vector (structural fingerprint)has\n",
    "HIDDEN_DIMENSION = 128 # How many nodes in the hidden layer of the neural network\n",
    "LAMBDA_PARAM = 5e-3 # lambda parameter for the Barlow Twins loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph generator: hyperedge, variable supercell size, next nearest neighbor and PBC, xyz node features, xyz edge attributes\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from ase.io import read\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Define the tolerance delta for nearest neighbors\n",
    "delta = 0.1 \n",
    "\n",
    "# Define the perturbation size\n",
    "perturbation_size = 0.05 \n",
    "\n",
    "# Function to compute edge_index based on positions and delta, considering PBCs\n",
    "def compute_edge_index(atoms, delta):\n",
    "    positions = atoms.get_positions()\n",
    "    num_atoms = len(positions)\n",
    "\n",
    "    # Compute distance matrix considering PBCs\n",
    "    dist_matrix = atoms.get_all_distances(mic=True)\n",
    "    edge_index_set = set()\n",
    "\n",
    "    # For each node, find its nearest neighbors and next-nearest neighbors\n",
    "    for i in range(num_atoms):\n",
    "        # Exclude self-distance by setting diagonal to infinity\n",
    "        dist_matrix[i, i] = np.inf\n",
    "\n",
    "        # Get distances and sort them along with indices\n",
    "        distances = dist_matrix[i]\n",
    "        sorted_indices = np.argsort(distances)\n",
    "        sorted_distances = distances[sorted_indices]\n",
    "\n",
    "        # Nearest neighbor distance\n",
    "        d1 = sorted_distances[0]\n",
    "        nn_cutoff = d1 + delta\n",
    "\n",
    "        # Indices of nearest neighbors within d1 + delta\n",
    "        nn_indices = sorted_indices[sorted_distances <= nn_cutoff]\n",
    "\n",
    "        # Add edges from i to nearest neighbors\n",
    "        for j in nn_indices:\n",
    "            edge_index_set.add((i, j))\n",
    "\n",
    "        # Exclude nearest neighbors from consideration for next-nearest neighbors\n",
    "        remaining_indices = sorted_indices[sorted_distances > nn_cutoff]\n",
    "        remaining_distances = sorted_distances[sorted_distances > nn_cutoff]\n",
    "\n",
    "        if len(remaining_distances) > 0:\n",
    "            # Next-nearest neighbor distance\n",
    "            d2 = remaining_distances[0]\n",
    "            nnn_cutoff = d2 + delta\n",
    "\n",
    "            # Indices of next-nearest neighbors within d2 + delta\n",
    "            nnn_indices = remaining_indices[remaining_distances <= nnn_cutoff]\n",
    "\n",
    "            # Add edges from i to next-nearest neighbors\n",
    "            for j in nnn_indices:\n",
    "                edge_index_set.add((i, j))\n",
    "\n",
    "    # Convert edge_index_set to a tensor\n",
    "    if len(edge_index_set) > 0:\n",
    "        edge_index = torch.tensor(list(edge_index_set), dtype=torch.long).t().contiguous()\n",
    "    else:\n",
    "        # Handle graphs with no edges\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "    # Make edges undirected by adding reverse edges\n",
    "    edge_index_rev = edge_index.flip(0)\n",
    "    edge_index = torch.cat([edge_index, edge_index_rev], dim=1)\n",
    "\n",
    "    return edge_index\n",
    "\n",
    "# Function to compute edge attributes as displacement vectors considering PBCs\n",
    "def compute_edge_attr(atoms, edge_index):\n",
    "    row, col = edge_index\n",
    "    positions = atoms.get_positions()\n",
    "    cell = atoms.get_cell()\n",
    "    scaled_positions = atoms.get_scaled_positions()\n",
    "\n",
    "    # Compute displacement vectors considering PBCs\n",
    "    delta_scaled = scaled_positions[row.numpy()] - scaled_positions[col.numpy()]\n",
    "    delta_scaled -= np.round(delta_scaled)  # Apply minimum image convention\n",
    "    displacement_vectors = delta_scaled @ cell  # Convert back to Cartesian coordinates\n",
    "\n",
    "    # Create edge attributes as displacement vectors\n",
    "    edge_attr = torch.tensor(displacement_vectors, dtype=torch.float)\n",
    "    return edge_attr\n",
    "\n",
    "# Function to compute node features (relative positions within unit cells)\n",
    "def compute_node_features(atoms, SUPERCELL_SIZE):\n",
    "    num_atoms = len(atoms)\n",
    "    N = SUPERCELL_SIZE\n",
    "    total_unit_cells = N * N \n",
    "\n",
    "    # Compute the number of atoms per unit cell\n",
    "    atoms_per_unit_cell = num_atoms // total_unit_cells\n",
    "\n",
    "    # Check for consistency\n",
    "    if atoms_per_unit_cell * total_unit_cells != num_atoms:\n",
    "        raise ValueError(\"Number of atoms per unit cell is not consistent with total atoms and supercell dimensions.\")\n",
    "\n",
    "    # Group atoms by unit cell based on their order in the .xyz file\n",
    "    unit_cell_indices = []\n",
    "    for i in range(0, num_atoms, atoms_per_unit_cell):\n",
    "        group = list(range(i, i + atoms_per_unit_cell))\n",
    "        unit_cell_indices.append(group)\n",
    "\n",
    "    # Compute relative positions\n",
    "    positions = atoms.get_positions()\n",
    "    relative_positions = np.zeros_like(positions)\n",
    "\n",
    "    for group in unit_cell_indices:\n",
    "        indices = group\n",
    "        positions_in_cell = positions[indices]\n",
    "        reference_position = positions_in_cell[0]  # First atom in the unit cell\n",
    "        relative_positions_in_cell = positions_in_cell - reference_position\n",
    "        relative_positions[indices] = relative_positions_in_cell\n",
    "\n",
    "    # Create node features as torch tensor\n",
    "    node_features = torch.tensor(relative_positions, dtype=torch.float)\n",
    "    return node_features\n",
    "\n",
    "# Function to compute hyperedges based on existing edges\n",
    "def compute_hyperedges(edge_index, num_nodes):\n",
    "    # Build adjacency list\n",
    "    adj_list = [[] for _ in range(num_nodes)]\n",
    "    for idx in range(edge_index.size(1)):\n",
    "        src = edge_index[0, idx].item()\n",
    "        tgt = edge_index[1, idx].item()\n",
    "        adj_list[src].append(tgt)\n",
    "\n",
    "    # Remove duplicates in adjacency lists\n",
    "    for neighbors in adj_list:\n",
    "        neighbors[:] = list(set(neighbors))\n",
    "\n",
    "    # Create hyperedges\n",
    "    hyperedges = []\n",
    "    for j in range(num_nodes):\n",
    "        neighbors = adj_list[j]\n",
    "        # For all pairs of neighbors of node j\n",
    "        for idx1 in range(len(neighbors)):\n",
    "            for idx2 in range(idx1 + 1, len(neighbors)):\n",
    "                i = neighbors[idx1]\n",
    "                k = neighbors[idx2]\n",
    "                hyperedges.append([i, j, k])\n",
    "\n",
    "    if len(hyperedges) > 0:\n",
    "        hyperedge_index = torch.tensor(hyperedges, dtype=torch.long).t().contiguous()  # Shape [3, num_hyperedges]\n",
    "    else:\n",
    "        hyperedge_index = torch.empty((3, 0), dtype=torch.long)\n",
    "\n",
    "    return hyperedge_index\n",
    "\n",
    "# Function to compute hyperedge attributes (angles between edges)\n",
    "def compute_hyperedge_attr(atoms, hyperedge_index):\n",
    "    positions = atoms.get_positions()\n",
    "    cell = atoms.get_cell()\n",
    "    scaled_positions = atoms.get_scaled_positions()\n",
    "    num_hyperedges = hyperedge_index.size(1)\n",
    "    hyperedge_attr = []\n",
    "\n",
    "    for idx in range(num_hyperedges):\n",
    "        i = hyperedge_index[0, idx].item()\n",
    "        j = hyperedge_index[1, idx].item()\n",
    "        k = hyperedge_index[2, idx].item()\n",
    "\n",
    "        # Scaled positions\n",
    "        pos_i = scaled_positions[i]\n",
    "        pos_j = scaled_positions[j]\n",
    "        pos_k = scaled_positions[k]\n",
    "\n",
    "        # Displacement vectors considering PBCs\n",
    "        delta_ji_scaled = pos_i - pos_j\n",
    "        delta_ji_scaled -= np.round(delta_ji_scaled)\n",
    "        d_ji = delta_ji_scaled @ cell\n",
    "\n",
    "        delta_jk_scaled = pos_k - pos_j\n",
    "        delta_jk_scaled -= np.round(delta_jk_scaled)\n",
    "        d_jk = delta_jk_scaled @ cell\n",
    "\n",
    "        # Compute angle between d_ji and d_jk\n",
    "        cos_theta = np.dot(d_ji, d_jk) / (np.linalg.norm(d_ji) * np.linalg.norm(d_jk) + 1e-8)\n",
    "        angle = np.arccos(np.clip(cos_theta, -1.0, 1.0))\n",
    "\n",
    "        hyperedge_attr.append([angle])\n",
    "\n",
    "    hyperedge_attr = torch.tensor(hyperedge_attr, dtype=torch.float)\n",
    "    return hyperedge_attr\n",
    "\n",
    "# Function to create a PyTorch Geometric Data object from an atomic structure\n",
    "def create_graph_from_structure(atoms, delta, SUPERCELL_SIZE):\n",
    "    num_atoms = len(atoms)\n",
    "\n",
    "    # Compute edge_index\n",
    "    edge_index = compute_edge_index(atoms, delta)\n",
    "\n",
    "    # Compute edge attributes\n",
    "    edge_attr = compute_edge_attr(atoms, edge_index)\n",
    "\n",
    "    # Compute node features\n",
    "    node_features = compute_node_features(atoms, SUPERCELL_SIZE)\n",
    "\n",
    "    # Compute hyperedges\n",
    "    hyperedge_index = compute_hyperedges(edge_index, num_atoms)\n",
    "\n",
    "    # Compute hyperedge attributes\n",
    "    hyperedge_attr = compute_hyperedge_attr(atoms, hyperedge_index)\n",
    "\n",
    "    # Create a PyTorch Geometric Data object\n",
    "    graph = Data(\n",
    "        x=node_features,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        hyperedge_index=hyperedge_index,\n",
    "        hyperedge_attr=hyperedge_attr,\n",
    "    )\n",
    "\n",
    "    graph.supercell_size = SUPERCELL_SIZE  # Store supercell size in graph\n",
    "\n",
    "    return graph\n",
    "\n",
    "# Function to apply perturbations to a graph list\n",
    "def perturb_graphs(graph_list, atoms_list, perturbation_size, delta):\n",
    "    perturbed_graph_list = []\n",
    "    for graph, atoms in zip(graph_list, atoms_list):\n",
    "        # Clone the graph to avoid modifying the original\n",
    "        perturbed_graph = graph.clone()\n",
    "\n",
    "        # Get the positions of the atoms from the atoms object\n",
    "        positions = atoms.get_positions().copy()  # Original positions\n",
    "\n",
    "        # Apply random perturbation to the positions\n",
    "        if perturbation_size > 0:\n",
    "            perturbation = np.random.uniform(-perturbation_size, perturbation_size, positions.shape)\n",
    "            positions += perturbation  # Perturb the atomic positions\n",
    "\n",
    "        # Update atoms object with perturbed positions\n",
    "        perturbed_atoms = atoms.copy()\n",
    "        perturbed_atoms.set_positions(positions)\n",
    "        perturbed_atoms.pbc = [True, True, False]  # Ensure PBCs are enabled\n",
    "\n",
    "        edge_index = graph.edge_index\n",
    "\n",
    "        # Recompute edge attributes\n",
    "        perturbed_edge_attr = compute_edge_attr(perturbed_atoms, edge_index)\n",
    "        perturbed_graph.edge_attr = perturbed_edge_attr\n",
    "\n",
    "        # Recompute node features based on perturbed positions\n",
    "        perturbed_node_features = compute_node_features(perturbed_atoms, graph.supercell_size)\n",
    "        perturbed_graph.x = perturbed_node_features\n",
    "\n",
    "        # Recompute hyperedge attributes based on perturbed positions\n",
    "        hyperedge_index = graph.hyperedge_index\n",
    "        perturbed_hyperedge_attr = compute_hyperedge_attr(perturbed_atoms, hyperedge_index)\n",
    "        perturbed_graph.hyperedge_attr = perturbed_hyperedge_attr\n",
    "\n",
    "        perturbed_graph_list.append(perturbed_graph)\n",
    "\n",
    "    return perturbed_graph_list\n",
    "\n",
    "# Function to read graphs and atoms from a folder\n",
    "def read_graphs_from_folder(folder_path, delta, SUPERCELL_SIZE):\n",
    "    graph_list = []\n",
    "    atoms_list = []\n",
    "    filenames = sorted([f for f in os.listdir(folder_path) if f.endswith('.xyz')])\n",
    "    for filename in filenames:\n",
    "        # Read the structure file\n",
    "        filepath = os.path.join(folder_path, filename)\n",
    "        atoms = read(filepath)\n",
    "        atoms.pbc = [True, True, False]  # Ensure PBCs are enabled\n",
    "\n",
    "        # Extract the last number from the filename\n",
    "        match = re.findall(r'\\d+', filename)\n",
    "        if match:\n",
    "            label = int(match[-1])  # Get the last number\n",
    "        else:\n",
    "            label = None  # Handle cases where no number is found\n",
    "\n",
    "        # Create graph with PBCs\n",
    "        graph = create_graph_from_structure(atoms, delta, SUPERCELL_SIZE)\n",
    "        graph.label = label\n",
    "\n",
    "        # Append the graph and atoms to their respective lists\n",
    "        graph_list.append(graph)\n",
    "        atoms_list.append(atoms)\n",
    "\n",
    "        print(f\"Graph for {filename} created and added to the list with label {label}.\")\n",
    "\n",
    "    return graph_list, atoms_list\n",
    "\n",
    "# Paths to the supercell files folders\n",
    "supercell_folder_1 = 'supercells_flatband_rotated_shifted_aligned_3x3'\n",
    "supercell_folder_2 = 'supercells_flatband_rotated_shifted_aligned_4x4'\n",
    "\n",
    "SUPERCELL_SIZE_1 = 3  # Adjust as needed\n",
    "SUPERCELL_SIZE_2 = 4  # Adjust as needed\n",
    "\n",
    "# Read graphs and atoms from the 3x3 folder\n",
    "graph_list_unperturbed_1, atoms_list_unperturbed_1 = read_graphs_from_folder(\n",
    "    supercell_folder_1, delta, SUPERCELL_SIZE_1\n",
    ")\n",
    "print(\"Unperturbed graph list for 3x3 supercells created with periodic boundary conditions accounted for.\")\n",
    "\n",
    "# Read graphs and atoms from the 4x4 folder\n",
    "graph_list_unperturbed_2, atoms_list_unperturbed_2 = read_graphs_from_folder(\n",
    "    supercell_folder_2, delta, SUPERCELL_SIZE_2\n",
    ")\n",
    "print(\"Unperturbed graph list for 4x4 supercells created with periodic boundary conditions accounted for.\")\n",
    "\n",
    "# Perturb the unperturbed 3x3 graphs to create the first perturbed graph list\n",
    "graph_list_set_1 = perturb_graphs(\n",
    "    graph_list_unperturbed_1, atoms_list_unperturbed_1, perturbation_size, delta\n",
    ")\n",
    "\n",
    "# Perturb the unperturbed 4x4 graphs to create the second perturbed graph list\n",
    "graph_list_set_2 = perturb_graphs(\n",
    "    graph_list_unperturbed_2, atoms_list_unperturbed_2, perturbation_size, delta\n",
    ")\n",
    "\n",
    "print(\"Two perturbed graph lists created from 3x3 and 4x4 supercells respectively.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder: Hyperedge\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
    "from torch_geometric.utils import add_self_loops\n",
    "from torch_geometric.nn.norm import LayerNorm\n",
    "\n",
    "class HyperedgeGNNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, edge_dim, hyperedge_dim, aggr='add'):\n",
    "        super(HyperedgeGNNConv, self).__init__(aggr=aggr)\n",
    "\n",
    "        # Linear transformations for node features\n",
    "        self.lin_node = nn.Linear(in_channels, out_channels)\n",
    "\n",
    "        # Linear transformations for edge messages\n",
    "        self.lin_edge = nn.Linear(in_channels + edge_dim, out_channels)\n",
    "\n",
    "        # Linear transformations for hyperedge messages\n",
    "        self.lin_hyperedge = nn.Linear(in_channels + hyperedge_dim, out_channels)\n",
    "\n",
    "        # Activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, hyperedge_index, hyperedge_attr):\n",
    "        # x: Node features [num_nodes, in_channels]\n",
    "        # edge_index: Edge indices [2, num_edges]\n",
    "        # edge_attr: Edge attributes [num_edges, edge_dim]\n",
    "        # hyperedge_index: Hyperedge indices [3, num_hyperedges]\n",
    "        # hyperedge_attr: Hyperedge attributes [num_hyperedges, hyperedge_dim]\n",
    "\n",
    "        # Initial node feature transformation\n",
    "        x = self.lin_node(x)\n",
    "\n",
    "        # Message passing for edges\n",
    "        edge_messages = self.propagate(\n",
    "            edge_index, x=x, edge_attr=edge_attr, mode='edge'\n",
    "        )\n",
    "\n",
    "        # Message passing for hyperedges\n",
    "        # For hyperedges, we need to reshape the hyperedge_index to simulate pairwise edges\n",
    "        # We'll process hyperedges by sending messages from hyperedge nodes to central node\n",
    "\n",
    "        # Expand hyperedges into pairwise edges\n",
    "        num_hyperedges = hyperedge_index.size(1)\n",
    "        # For each hyperedge (i, j, k), create edges (i -> j), (k -> j), and (j -> j)\n",
    "        sender_indices = torch.cat([hyperedge_index[0], hyperedge_index[2], hyperedge_index[1]])\n",
    "        receiver_indices = hyperedge_index[1].repeat(3)\n",
    "        hyperedge_edge_index = torch.stack([sender_indices, receiver_indices], dim=0)\n",
    "\n",
    "        # Repeat hyperedge attributes for each new edge\n",
    "        hyperedge_edge_attr = hyperedge_attr.repeat(3, 1)\n",
    "\n",
    "        hyperedge_messages = self.propagate(\n",
    "            hyperedge_edge_index, x=x, edge_attr=hyperedge_edge_attr, mode='hyperedge'\n",
    "        )\n",
    "\n",
    "        # Combine messages\n",
    "        out = x + edge_messages + hyperedge_messages\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j, edge_attr, mode):\n",
    "        if mode == 'edge':\n",
    "            # Edge message passing remains the same\n",
    "            msg_input = torch.cat([x_j, edge_attr], dim=-1)\n",
    "            msg = self.lin_edge(msg_input)\n",
    "        elif mode == 'hyperedge':\n",
    "            # For hyperedges, x_j represents features from hyperedge nodes\n",
    "            # Concatenate x_j with hyperedge attributes\n",
    "            msg_input = torch.cat([x_j, edge_attr], dim=-1)\n",
    "            msg = self.lin_hyperedge(msg_input)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode. Use 'edge' or 'hyperedge'.\")\n",
    "\n",
    "        return msg\n",
    "\n",
    "    def message_and_aggregate(self, adj_t):\n",
    "        # Not used in this implementation\n",
    "        pass\n",
    "\n",
    "class GNNWithHyperedges(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_node_features,\n",
    "        num_edge_features,\n",
    "        num_hyperedge_features,\n",
    "        hidden_dim=128,\n",
    "        embedding_dim=64,\n",
    "        num_layers=4,\n",
    "        dropout_rate=0.1,\n",
    "    ):\n",
    "        super(GNNWithHyperedges, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Initial linear transformation for node features\n",
    "        self.node_embedding = nn.Linear(num_node_features, hidden_dim)\n",
    "\n",
    "        # Lists to hold convolutional and normalization layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            conv = HyperedgeGNNConv(\n",
    "                in_channels=hidden_dim,\n",
    "                out_channels=hidden_dim,\n",
    "                edge_dim=num_edge_features,\n",
    "                hyperedge_dim=num_hyperedge_features,\n",
    "                aggr='mean',\n",
    "            )\n",
    "            self.convs.append(conv)\n",
    "            self.norms.append(LayerNorm(hidden_dim))\n",
    "\n",
    "        # Global mean pooling\n",
    "        self.pool = global_mean_pool\n",
    "\n",
    "        # Final MLP for graph-level embedding\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_rate),\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x  # Node features\n",
    "        edge_index = data.edge_index  # Edge indices\n",
    "        edge_attr = data.edge_attr  # Edge attributes\n",
    "        hyperedge_index = data.hyperedge_index  # Hyperedge indices\n",
    "        hyperedge_attr = data.hyperedge_attr  # Hyperedge attributes\n",
    "\n",
    "        # Ensure batch attribute is present\n",
    "        if hasattr(data, 'batch'):\n",
    "            batch = data.batch\n",
    "        else:\n",
    "            batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n",
    "\n",
    "        # Initial node embedding\n",
    "        x = self.node_embedding(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "\n",
    "        # Apply convolutional layers with residual connections\n",
    "        for conv, norm in zip(self.convs, self.norms):\n",
    "            x_residual = x  # Save residual\n",
    "            x = conv(x, edge_index, edge_attr, hyperedge_index, hyperedge_attr)\n",
    "            x = norm(x)\n",
    "            x = F.relu(x)\n",
    "            x = x + x_residual  # Residual connection\n",
    "            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "\n",
    "        # Global mean pooling to get graph-level representation\n",
    "        x = self.pool(x, batch)\n",
    "\n",
    "        # Final MLP layers to get embedding\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss function: barlow twins\n",
    "\n",
    "def off_diagonal(x):\n",
    "    # Returns the off-diagonal elements of a square matrix x\n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    return x.flatten()[1:].view(n - 1, n + 1)[:, :-1].flatten()\n",
    "\n",
    "def barlow_twins_loss(z_a, z_b, lambd=LAMBDA_PARAM):\n",
    "    \"\"\"\n",
    "    Computes the Barlow Twins loss between two sets of embeddings.\n",
    "    Args:\n",
    "        z_a: Embeddings from the first set (BATCH_SIZE x embedding_dim)\n",
    "        z_b: Embeddings from the second set (BATCH_SIZE x embedding_dim)\n",
    "        lambd: Balancing parameter for off-diagonal terms\n",
    "    Returns:\n",
    "        loss: Scalar tensor representing the loss\n",
    "    \"\"\"\n",
    "    # Normalize the embeddings along the batch dimension\n",
    "    N, D = z_a.size()\n",
    "    \n",
    "    # Subtract mean and divide by standard deviation\n",
    "    z_a_norm = (z_a - z_a.mean(dim=0)) / (z_a.std(dim=0))\n",
    "    z_b_norm = (z_b - z_b.mean(dim=0)) / (z_b.std(dim=0))\n",
    "    \n",
    "    # Compute the cross-correlation matrix\n",
    "    c = torch.mm(z_a_norm.T, z_b_norm) / N  # D x D matrix\n",
    "    \n",
    "    # Loss terms\n",
    "    on_diag = torch.diagonal(c).add_(-1).pow_(2).sum()\n",
    "    off_diag = off_diagonal(c).pow_(2).sum()\n",
    "    loss = on_diag + lambd * off_diag\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader code with necessary adjustments\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "# Define constants\n",
    "NUM_TRAINING = int(len(graph_list_set_1) * 0.8)  # 80% for training\n",
    "NUM_VALIDATE = len(graph_list_set_1) - NUM_TRAINING  # Remaining for validation\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "fixed_seed = 42\n",
    "np.random.seed(fixed_seed)\n",
    "indices = np.random.permutation(len(graph_list_set_1))\n",
    "\n",
    "# Shuffle the graph lists\n",
    "shuffled_graph_list_set_1 = [graph_list_set_1[i] for i in indices]\n",
    "shuffled_graph_list_set_2 = [graph_list_set_2[i] for i in indices]\n",
    "\n",
    "# Split the datasets into training and validation sets\n",
    "train_graph_list_set_1 = shuffled_graph_list_set_1[:NUM_TRAINING]\n",
    "val_graph_list_set_1 = shuffled_graph_list_set_1[NUM_TRAINING:]\n",
    "\n",
    "train_graph_list_set_2 = shuffled_graph_list_set_2[:NUM_TRAINING]\n",
    "val_graph_list_set_2 = shuffled_graph_list_set_2[NUM_TRAINING:]\n",
    "\n",
    "# Create DataLoaders for training and validation sets\n",
    "train_loader_set_1 = DataLoader(train_graph_list_set_1, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader_set_1 = DataLoader(val_graph_list_set_1, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "train_loader_set_2 = DataLoader(train_graph_list_set_2, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader_set_2 = DataLoader(val_graph_list_set_2, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"Data loaders created for training and validation sets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer adjusted for hyperedge GNN model\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(\"Models\", exist_ok=True)\n",
    "os.makedirs(\"Training Plots\", exist_ok=True)\n",
    "\n",
    "print(f\"\"\"\n",
    "-----------training starting---------------\n",
    "BATCH_SIZE: {BATCH_SIZE}\n",
    "NUM_EPOCHS: {NUM_EPOCHS}\n",
    "Learning Rate: {LEARNING_RATE}\n",
    "Hidden Dimension: {HIDDEN_DIMENSION}\n",
    "Embedding Dimension: {EMBEDDING_DIMENSION}\n",
    "Lambda (Barlow Twins Loss): {LAMBDA_PARAM}\n",
    "\"\"\")\n",
    "\n",
    "# Initialize the model with required arguments\n",
    "model = GNNWithHyperedges(\n",
    "    num_node_features=graph_list_set_1[0].x.size(1),  # Adjust based on node features\n",
    "    num_edge_features=graph_list_set_1[0].edge_attr.size(1),  # Adjust based on edge features\n",
    "    num_hyperedge_features=graph_list_set_1[0].hyperedge_attr.size(1),  # Adjust based on hyperedge features\n",
    "    hidden_dim=HIDDEN_DIMENSION,\n",
    "    embedding_dim=EMBEDDING_DIMENSION,\n",
    "    num_layers=2,  # Customize as needed\n",
    "    dropout_rate=0.1,\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "\n",
    "def compute_loss(model, data1, data2, lambd):\n",
    "    data1, data2 = data1.to(device), data2.to(device)\n",
    "    embedding1 = model(data1)\n",
    "    embedding2 = model(data2)\n",
    "    loss = barlow_twins_loss(embedding1, embedding2, lambd)\n",
    "    return loss\n",
    "\n",
    "def train_one_epoch(model, loader_set_1, loader_set_2, optimizer, lambd):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for data1, data2 in zip(loader_set_1, loader_set_2):\n",
    "        optimizer.zero_grad()\n",
    "        loss = compute_loss(model, data1, data2, lambd)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(loader_set_1)\n",
    "    return avg_train_loss\n",
    "\n",
    "def validate_model(model, loader_set_1, loader_set_2, lambd):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data1, data2 in zip(loader_set_1, loader_set_2):\n",
    "            loss = compute_loss(model, data1, data2, lambd)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_loss / len(loader_set_1)\n",
    "    return avg_val_loss\n",
    "\n",
    "def train_model(model, train_loader_1, train_loader_2, val_loader_1, val_loader_2, optimizer, num_epochs, lambd):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')  # Initialize best validation loss\n",
    "\n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            # Training phase\n",
    "            avg_train_loss = train_one_epoch(model, train_loader_1, train_loader_2, optimizer, lambd)\n",
    "            train_losses.append(avg_train_loss)\n",
    "\n",
    "            # Validation phase\n",
    "            avg_val_loss = validate_model(model, val_loader_1, val_loader_2, lambd)\n",
    "            val_losses.append(avg_val_loss)\n",
    "\n",
    "            # Check if this is the best validation loss so far\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                # Save the model\n",
    "                torch.save(model.state_dict(), f'Models/{MODEL_NAME}.pth')\n",
    "                print(f\"New best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "            # Print epoch information\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted by user. Best model saved so far.\")\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss', marker='o')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss', marker='o')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"Training Plots/{MODEL_NAME}.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Run the training and validation process\n",
    "train_losses, val_losses = train_model(\n",
    "    model, \n",
    "    train_loader_set_1, train_loader_set_2, \n",
    "    val_loader_set_1, val_loader_set_2, \n",
    "    optimizer, \n",
    "    NUM_EPOCHS, \n",
    "    LAMBDA_PARAM\n",
    ")\n",
    "\n",
    "plot_losses(train_losses, val_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Inspecting weights\n",
    "print(\"Lin Edge Weights:\", model.convs[0].lin_edge.weight)\n",
    "print(\"Lin Hyperedge Weights:\", model.convs[0].lin_hyperedge.weight)\n",
    "print(\"Lin Node Weights:\", model.convs[0].lin_node.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generates and saves embeddings\n",
    "\n",
    "def test_model(model, loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    embeddings_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculations\n",
    "        for batch_idx, data in enumerate(loader):\n",
    "            data = data.to(device)  # Move the batch to the appropriate device (CPU/GPU)\n",
    "\n",
    "            # Get embeddings for the batch\n",
    "            embeddings = model(data)  # Shape: (batch_size, embedding_dim)\n",
    "\n",
    "            # Store the embeddings for each graph in the batch\n",
    "            embeddings_list.append(embeddings)\n",
    "\n",
    "            # Extract labels from the batch and store them\n",
    "            labels = data.label  # Assuming 'label' is the attribute name\n",
    "            labels_list.append(labels)\n",
    "\n",
    "    # Concatenate all the embeddings and labels for the entire dataset\n",
    "    all_embeddings = torch.cat(embeddings_list, dim=0)  # Shape: (num_graphs, embedding_dim)\n",
    "    all_labels = torch.cat(labels_list, dim=0)          # Shape: (num_graphs,)\n",
    "    \n",
    "    return all_embeddings, all_labels\n",
    "\n",
    "# Assuming 'graph_list_original' contains graphs with 'label' attributes\n",
    "test_loader = DataLoader(graph_list_unperturbed_1, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Call the test function to generate embeddings and labels for the entire test set\n",
    "output_graph_embeddings, output_graph_labels = test_model(model, test_loader)\n",
    "\n",
    "# Print the shapes of the final embeddings and labels\n",
    "print(f\"Generated embeddings shape: {output_graph_embeddings.shape}\")\n",
    "print(f\"Corresponding labels shape: {output_graph_labels.shape}\")\n",
    "\n",
    "embeddings_path = f'Embeddings/embeddings_{MODEL_NAME}.pt'\n",
    "labels_path = f'Labels/labels_{MODEL_NAME}.pt'\n",
    "\n",
    "torch.save(output_graph_embeddings, embeddings_path)\n",
    "\n",
    "# Save the validation graph embeddings\n",
    "torch.save(output_graph_labels, labels_path)\n",
    "\n",
    "# Print confirmation message\n",
    "print(f\"embeddings saved to {embeddings_path}\")\n",
    "print(f\"labels saved to {labels_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering workflow\n",
    "#uses hdbscan to cluster embeddings and visualises the clusters use t-SNE and UMAP\n",
    "#also prints out addtional outlier score plots and other information\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import hdbscan\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "\n",
    "embeddings = torch.load(embeddings_path, map_location=torch.device('cpu')).numpy()\n",
    "labels = torch.load(labels_path, map_location=torch.device('cpu')).numpy()\n",
    "\n",
    "def hdbscan_clustering(data):\n",
    "    # Initialize and fit HDBSCAN\n",
    "    hdb = hdbscan.HDBSCAN(min_samples=5, min_cluster_size=4, prediction_data=True)\n",
    "    cluster_labels = hdb.fit_predict(data)\n",
    "    \n",
    "    # Retrieve cluster stability scores and outlier scores\n",
    "    stability_scores = hdb.cluster_persistence_\n",
    "    outlier_scores = hdb.outlier_scores_\n",
    "    \n",
    "    # Print and return the scores\n",
    "    print(\"Cluster Stability Scores:\", stability_scores)\n",
    "    print(\"Outlier Scores (first 10):\", outlier_scores[:10])  # Printing first 10 for brevity\n",
    "    \n",
    "    return hdb, cluster_labels, stability_scores, outlier_scores\n",
    "\n",
    "def tsne_plot(data, cluster_labels):\n",
    "    tsne = TSNE(n_components=2, perplexity=20, random_state=42, init='pca')\n",
    "    data_tsne_2d = tsne.fit_transform(data)\n",
    "\n",
    "    unique_labels = np.unique(cluster_labels)\n",
    "    background_points = (cluster_labels == -1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(data_tsne_2d[background_points, 0], data_tsne_2d[background_points, 1],\n",
    "                c='lightgray', s=10, alpha=0.5, label='Noise')\n",
    "    \n",
    "    scatter = plt.scatter(data_tsne_2d[~background_points, 0], data_tsne_2d[~background_points, 1],\n",
    "                          c=cluster_labels[~background_points], cmap='tab20', s=10, alpha=0.7)\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        if label != -1:\n",
    "            label_points = data_tsne_2d[cluster_labels == label]\n",
    "            centroid = np.mean(label_points, axis=0)\n",
    "            plt.text(centroid[0], centroid[1], str(label), fontsize=8, fontweight='bold', \n",
    "                     color='black', ha='center', va='center')\n",
    "    \n",
    "    plt.title('t-SNE Visualization of HDBSCAN Clusters in 2D')\n",
    "    plt.xlabel('t-SNE 1')\n",
    "    plt.ylabel('t-SNE 2')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'Clustering Plots/t-SNE_clusters_{MODEL_NAME}.png')\n",
    "    plt.show()\n",
    "\n",
    "def umap_plot(data, cluster_labels):\n",
    "    umap_reducer = umap.UMAP(n_neighbors=20, n_components=2, min_dist=0.5 ,random_state=42, init='pca')\n",
    "    data_umap_2d = umap_reducer.fit_transform(data)\n",
    "\n",
    "    unique_labels = np.unique(cluster_labels)\n",
    "    background_points = (cluster_labels == -1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(data_umap_2d[background_points, 0], data_umap_2d[background_points, 1],\n",
    "                c='lightgray', s=10, alpha=0.5, label='Noise')\n",
    "    \n",
    "    scatter = plt.scatter(data_umap_2d[~background_points, 0], data_umap_2d[~background_points, 1],\n",
    "                          c=cluster_labels[~background_points], cmap='tab20', s=10, alpha=0.7)\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        if label != -1:\n",
    "            label_points = data_umap_2d[cluster_labels == label]\n",
    "            centroid = np.mean(label_points, axis=0)\n",
    "            plt.text(centroid[0], centroid[1], str(label), fontsize=8, fontweight='bold', \n",
    "                     color='black', ha='center', va='center')\n",
    "    \n",
    "    plt.title('UMAP Visualization of HDBSCAN Clusters in 2D')\n",
    "    plt.xlabel('UMAP 1')\n",
    "    plt.ylabel('UMAP 2')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'Clustering Plots/UMAP_clusters_{MODEL_NAME}')\n",
    "    plt.show()\n",
    "\n",
    "def save_clusters_with_labels(cluster_labels, embedding_labels, filename='clustered_labels.npy'):\n",
    "    combined_array = np.column_stack((embedding_labels, cluster_labels))\n",
    "    np.save(filename, combined_array)\n",
    "    print(f\"Clusters and labels saved to {filename}\")\n",
    "\n",
    "def full_clustering_workflow(data, embedding_labels):\n",
    "    print(\"Clustering with HDBSCAN...\")\n",
    "    hdb, cluster_labels, stability_scores, outlier_scores = hdbscan_clustering(data)\n",
    "    save_clusters_with_labels(cluster_labels, embedding_labels)\n",
    "    \n",
    "    # Separate calls to visualization functions\n",
    "    print(\"Visualizing with t-SNE...\")\n",
    "    tsne_plot(data, cluster_labels)\n",
    "    \n",
    "    print(\"Visualizing with UMAP...\")\n",
    "    umap_plot(data, cluster_labels)\n",
    "    \n",
    "    # Calculate and print average cluster stability score\n",
    "    avg_stability_score = np.mean(stability_scores)\n",
    "    print(f\"Average Cluster Stability Score: {avg_stability_score:.4f}\")\n",
    "    \n",
    "    # Masks\n",
    "    clustered_mask = (cluster_labels != -1)\n",
    "    noise_mask = (cluster_labels == -1)\n",
    "    \n",
    "    # Verify the number of clustered and noise points\n",
    "    num_clustered_points = np.sum(clustered_mask)\n",
    "    num_noise_points = np.sum(noise_mask)\n",
    "    \n",
    "    print(\"Total number of data points:\", len(cluster_labels))\n",
    "    print(\"Number of clustered points:\", num_clustered_points)\n",
    "    print(\"Number of noise points:\", num_noise_points)\n",
    "    \n",
    "    # Filter out NaN values from outlier scores\n",
    "    valid_indices = ~np.isnan(outlier_scores)\n",
    "    clustered_mask = clustered_mask & valid_indices\n",
    "    noise_mask = noise_mask & valid_indices\n",
    "    \n",
    "    clustered_outlier_scores = outlier_scores[clustered_mask]\n",
    "    noise_outlier_scores = outlier_scores[noise_mask]\n",
    "    \n",
    "    # Calculate and print average outlier scores\n",
    "    if clustered_outlier_scores.size > 0:\n",
    "        avg_clustered_outlier_score = np.mean(clustered_outlier_scores)\n",
    "        print(f\"Average Outlier Score for Clustered Points: {avg_clustered_outlier_score:.4f}\")\n",
    "    else:\n",
    "        print(\"No valid outlier scores for clustered points.\")\n",
    "    \n",
    "    if noise_outlier_scores.size > 0:\n",
    "        avg_noise_outlier_score = np.mean(noise_outlier_scores)\n",
    "        print(f\"Average Outlier Score for Noise Points: {avg_noise_outlier_score:.4f}\")\n",
    "    else:\n",
    "        print(\"No valid outlier scores for noise points.\")\n",
    "    \n",
    "    # Plot histograms\n",
    "    # Clustered points\n",
    "    if clustered_outlier_scores.size > 0:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.hist(clustered_outlier_scores, bins=20, color='skyblue', edgecolor='black')\n",
    "        plt.title('Outlier Scores Distribution for Clustered Points')\n",
    "        plt.xlabel('Outlier Score')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No valid outlier scores for clustered points to plot.\")\n",
    "    \n",
    "    # Noise points\n",
    "    if noise_outlier_scores.size > 0:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.hist(noise_outlier_scores, bins=20, color='salmon', edgecolor='black')\n",
    "        plt.title('Outlier Scores Distribution for Noise Points')\n",
    "        plt.xlabel('Outlier Score')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.show()\n",
    "    \n",
    "    return hdb, cluster_labels\n",
    "\n",
    "def load_clusters_with_labels(filename='clustered_labels.npy'):\n",
    "    # Load the saved numpy array\n",
    "    loaded_array = np.load(filename)\n",
    "    \n",
    "    # Split the loaded array into embedding labels and cluster labels\n",
    "    embedding_labels = loaded_array[:, 0]  # First column is embedding labels\n",
    "    cluster_labels = loaded_array[:, 1]    # Second column is cluster labels\n",
    "    \n",
    "    print(f\"Clusters and labels loaded from {filename}\")\n",
    "    return embedding_labels, cluster_labels\n",
    "\n",
    "# Assuming 'embeddings' and 'labels' are already defined in your workspace\n",
    "hdb, cluster_labels = full_clustering_workflow(embeddings[:4000], labels[:4000])\n",
    "\n",
    "# Load embedding_labels from the saved file\n",
    "embedding_labels, _ = load_clusters_with_labels('clustered_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Provides the properties of and visualizes every node in the specified cluster, including the full lattice formula\n",
    "\n",
    "CLUSTER_NUM = 48\n",
    "cluster = embedding_labels[cluster_labels==CLUSTER_NUM]\n",
    "cluster\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from ase.io import read\n",
    "from torch_geometric.data import Data\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Define the tolerance delta for nearest neighbors\n",
    "delta = 0.1  # Adjust as needed\n",
    "\n",
    "# Global parameter for the supercell size\n",
    "SUPERCELL_SIZE = 3\n",
    "\n",
    "# Define the folder to save plots\n",
    "plot_folder = 'cluster_plots'\n",
    "\n",
    "# Ensure the plot folder exists\n",
    "if not os.path.exists(plot_folder):\n",
    "    os.makedirs(plot_folder)\n",
    "else:\n",
    "    # Delete all files in the directory\n",
    "    for filename in os.listdir(plot_folder):\n",
    "        file_path = os.path.join(plot_folder, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)  # Remove the file\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)  # Remove the directory\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {file_path}. Reason: {e}\")\n",
    "\n",
    "# Function to parse the reduced_sublattice_structure.txt file\n",
    "def parse_reduced_sublattice_structure(file_path):\n",
    "    material_formula_mapping = {}\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        material_label = None\n",
    "        for i, line in enumerate(lines):\n",
    "            line = line.strip()\n",
    "            if line.startswith('2dm-'):\n",
    "                # Extract the material label number\n",
    "                material_label = int(line.split('-')[1])\n",
    "                # The next line contains the full lattice formula\n",
    "                if i + 1 < len(lines):\n",
    "                    next_line = lines[i + 1].strip()\n",
    "                    # Extract the full lattice formula (e.g., 'TaI3')\n",
    "                    full_lattice_formula = next_line.split()[0]\n",
    "                    material_formula_mapping[material_label] = full_lattice_formula\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while parsing the file: {e}\")\n",
    "    return material_formula_mapping\n",
    "\n",
    "# Function to compute edge_index based on positions and delta, considering PBCs\n",
    "def compute_edge_index(atoms, delta):\n",
    "    positions = atoms.get_positions()\n",
    "    num_atoms = len(positions)\n",
    "\n",
    "    # Compute distance matrix considering PBCs\n",
    "    dist_matrix = atoms.get_all_distances(mic=True)\n",
    "    edge_index = set()  # Use a set to avoid duplicate edges\n",
    "\n",
    "    # For each node, find its nearest neighbors and next-nearest neighbors\n",
    "    for i in range(num_atoms):\n",
    "        # Exclude self-distance by setting diagonal to infinity\n",
    "        dist_matrix[i, i] = np.inf\n",
    "\n",
    "        # Get distances and sort them along with indices\n",
    "        distances = dist_matrix[i]\n",
    "        sorted_indices = np.argsort(distances)\n",
    "        sorted_distances = distances[sorted_indices]\n",
    "\n",
    "        # Nearest neighbor distance\n",
    "        d1 = sorted_distances[0]\n",
    "        nn_cutoff = d1 + delta\n",
    "\n",
    "        # Indices of nearest neighbors within d1 + delta\n",
    "        nn_indices = sorted_indices[sorted_distances <= nn_cutoff]\n",
    "\n",
    "        # Add edges from nearest neighbors to i\n",
    "        for j in nn_indices:\n",
    "            edge_index.add((j, i))\n",
    "\n",
    "        # Exclude nearest neighbors from consideration for next-nearest neighbors\n",
    "        remaining_indices = sorted_indices[sorted_distances > nn_cutoff]\n",
    "        remaining_distances = sorted_distances[sorted_distances > nn_cutoff]\n",
    "\n",
    "        if len(remaining_distances) > 0:\n",
    "            # Next-nearest neighbor distance\n",
    "            d2 = remaining_distances[0]\n",
    "            nnn_cutoff = d2 + delta\n",
    "\n",
    "            # Indices of next-nearest neighbors within d2 + delta\n",
    "            nnn_indices = remaining_indices[remaining_distances <= nnn_cutoff]\n",
    "\n",
    "            # Add edges from next-nearest neighbors to i\n",
    "            for j in nnn_indices:\n",
    "                edge_index.add((j, i))\n",
    "\n",
    "    # Convert edge_index to a tensor\n",
    "    if len(edge_index) > 0:\n",
    "        edge_index = torch.tensor(list(edge_index), dtype=torch.long).t().contiguous()\n",
    "    else:\n",
    "        # Handle graphs with no edges\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "    return edge_index\n",
    "\n",
    "# Function to compute edge attributes based on positions and edge_index, considering PBCs\n",
    "def compute_edge_attr(atoms, edge_index):\n",
    "    row, col = edge_index\n",
    "    positions = atoms.get_positions()\n",
    "    cell = atoms.get_cell()\n",
    "    scaled_positions = atoms.get_scaled_positions()\n",
    "\n",
    "    # Compute displacement vectors considering PBCs\n",
    "    delta_scaled = scaled_positions[row.numpy()] - scaled_positions[col.numpy()]\n",
    "    delta_scaled -= np.round(delta_scaled)\n",
    "    displacement_vectors = delta_scaled @ cell\n",
    "    edge_distances = np.linalg.norm(displacement_vectors, axis=1)\n",
    "    edge_attr = torch.tensor(edge_distances, dtype=torch.float).unsqueeze(1)\n",
    "    return edge_attr\n",
    "\n",
    "# Function to create a PyTorch Geometric Data object from an atomic structure\n",
    "def create_graph_from_structure(atoms, delta):\n",
    "    # Compute edge_index\n",
    "    edge_index = compute_edge_index(atoms, delta)\n",
    "\n",
    "    # Compute edge attributes\n",
    "    edge_attr = compute_edge_attr(atoms, edge_index)\n",
    "\n",
    "    # No node features\n",
    "    num_nodes = len(atoms)\n",
    "    node_features = torch.empty((num_nodes, 0), dtype=torch.float)\n",
    "\n",
    "    # Create a PyTorch Geometric Data object\n",
    "    graph = Data(\n",
    "        x=node_features,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "    )\n",
    "\n",
    "    return graph\n",
    "\n",
    "# Function to plot the graph with consistent y-size\n",
    "def plot_graph(atoms, graph, margin=0.1, tolerance=0.1, y_size=6, filename=None):\n",
    "    \"\"\"\n",
    "    Plots the graph of an atomic structure with consistent y-direction size.\n",
    "\n",
    "    Parameters:\n",
    "    - atoms: ASE Atoms object representing the structure.\n",
    "    - graph: PyTorch Geometric Data object for the graph.\n",
    "    - margin: Margin added to the plot boundaries as a fraction of plot size.\n",
    "    - tolerance: Tolerance for grouping atoms into layers by z-coordinate.\n",
    "    - y_size: Desired size of the plot in the y-direction (in inches).\n",
    "    - filename: If provided, saves the plot to this file instead of showing it.\n",
    "    \"\"\"\n",
    "    positions = atoms.get_positions()\n",
    "    cell = atoms.get_cell()\n",
    "\n",
    "    # Extract x, y, and z positions\n",
    "    x = positions[:, 0]\n",
    "    y = positions[:, 1]\n",
    "    z = positions[:, 2]\n",
    "\n",
    "    # Group z positions into layers using the tolerance\n",
    "    z_grouped = np.round(z / tolerance) * tolerance\n",
    "\n",
    "    # Get unique z positions and assign an index to each layer\n",
    "    z_unique, indices = np.unique(z_grouped, return_inverse=True)\n",
    "    N_layers = len(z_unique)\n",
    "\n",
    "    # Define a colormap with a number of colors equal to the number of layers\n",
    "    cmap = plt.cm.get_cmap('viridis', N_layers)\n",
    "\n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Shift supercell boundary by 0.5c\n",
    "    c_shift = 0.5 * cell[2]  # Half the c vector\n",
    "    corner_positions = np.array([\n",
    "        [0, 0, 0],\n",
    "        cell[0],\n",
    "        cell[0] + cell[1],\n",
    "        cell[1],\n",
    "        [0, 0, 0]\n",
    "    ]) + c_shift  # Add the c-shift vector to each vertex\n",
    "\n",
    "    # Recalculate plot limits based on the shifted boundary\n",
    "    x_min, x_max = corner_positions[:, 0].min(), corner_positions[:, 0].max()\n",
    "    y_min, y_max = corner_positions[:, 1].min(), corner_positions[:, 1].max()\n",
    "\n",
    "    # Add margin to the limits\n",
    "    x_margin = (x_max - x_min) * margin\n",
    "    y_margin = (y_max - y_min) * margin\n",
    "\n",
    "    x_min -= x_margin\n",
    "    x_max += x_margin\n",
    "    y_min -= y_margin\n",
    "    y_max += y_margin\n",
    "\n",
    "    # Dynamically adjust the figure size to maintain consistent y-direction size\n",
    "    y_range = y_max - y_min\n",
    "    x_range = x_max - x_min\n",
    "    aspect_ratio = x_range / y_range\n",
    "\n",
    "    fig.set_size_inches(y_size * aspect_ratio, y_size)  # Scale x-size proportionally\n",
    "\n",
    "    # Plot the supercell boundary\n",
    "    ax.plot(corner_positions[:, 0], corner_positions[:, 1], 'k--', linewidth=1)\n",
    "\n",
    "    # Plot the nodes with colors based on z position\n",
    "    ax.scatter(x, y, c=indices, s=50, cmap=cmap, zorder=2)\n",
    "\n",
    "    # Prepare edge lines\n",
    "    lines = []\n",
    "    for idx in range(graph.edge_index.shape[1]):\n",
    "        i = graph.edge_index[0, idx].item()\n",
    "        j = graph.edge_index[1, idx].item()\n",
    "\n",
    "        pos_i = positions[i]\n",
    "        pos_j = positions[j]\n",
    "\n",
    "        # Compute delta_scaled considering PBCs\n",
    "        delta_scaled = atoms.get_scaled_positions()[j] - atoms.get_scaled_positions()[i]\n",
    "        delta_scaled -= np.round(delta_scaled)\n",
    "\n",
    "        # Adjusted position of j for plotting\n",
    "        delta = delta_scaled @ cell\n",
    "        pos_j_plot = pos_i + delta\n",
    "\n",
    "        # Add the line segment\n",
    "        lines.append([pos_i[:2], pos_j_plot[:2]])\n",
    "\n",
    "    # Create a LineCollection from the lines\n",
    "    lc = LineCollection(lines, colors='gray', linewidths=1, zorder=1)\n",
    "    ax.add_collection(lc)\n",
    "\n",
    "    # Set limits with new boundaries\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "\n",
    "    # Set aspect ratio to equal\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    # Remove axes and adjust layout\n",
    "    ax.axis('off')\n",
    "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "\n",
    "    # Save the plot to a file or show it\n",
    "    if filename:\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "# Function to process every node in the specified cluster and collect data\n",
    "def process_nodes_in_cluster(CLUSTER_NUM, hdb, cluster_labels, embedding_labels, formula_mapping):\n",
    "    # Find indices of points in the given cluster\n",
    "    cluster_indices = np.where(cluster_labels == CLUSTER_NUM)[0]\n",
    "\n",
    "    if len(cluster_indices) == 0:\n",
    "        print(f\"No points found in cluster {CLUSTER_NUM}.\")\n",
    "        return None\n",
    "\n",
    "    data_list = []\n",
    "\n",
    "    for idx in cluster_indices:\n",
    "        material_label = int(embedding_labels[idx])\n",
    "\n",
    "        # Build the file path\n",
    "        file_path = f'supercells_flatband_rotated_shifted_3x3_test/supercell_2dm-{material_label}.xyz'\n",
    "\n",
    "        # Get the full lattice formula from the mapping\n",
    "        full_lattice_formula = formula_mapping.get(material_label, 'Unknown')\n",
    "\n",
    "        # Process the structure\n",
    "        try:\n",
    "            atoms = read(file_path)\n",
    "            atoms.pbc = [True, True, False]  # Ensure PBCs are enabled\n",
    "\n",
    "            # Create graph with PBCs\n",
    "            graph = create_graph_from_structure(atoms, delta)\n",
    "\n",
    "            # Save the plot to a file in the plot_folder\n",
    "            plot_filename = f'graph_cluster_{CLUSTER_NUM}_material_{material_label}_idx_{idx}.png'\n",
    "            plot_filepath = os.path.join(plot_folder, plot_filename)\n",
    "            plot_graph(atoms, graph, margin=0.2, tolerance=0.1, y_size=6, filename=plot_filepath)\n",
    "            print(f\"Graph for material label {material_label} from cluster {CLUSTER_NUM} saved as {plot_filepath}.\")\n",
    "\n",
    "            # Extract additional properties\n",
    "            num_atoms = len(atoms)  # Number of atoms\n",
    "            cell_lengths = atoms.cell.diagonal()  # Get lengths of the cell\n",
    "            cell_angles = atoms.cell.cellpar()[3:6]  # Get angles in degrees\n",
    "\n",
    "            # Scale the first two cell lengths by dividing by SUPERCELL_SIZE\n",
    "            scaled_cell_lengths = [\n",
    "                f'{cell_lengths[0] / SUPERCELL_SIZE:.3f}',\n",
    "                f'{cell_lengths[1] / SUPERCELL_SIZE:.3f}',\n",
    "                f'{cell_lengths[2]:.3f}'\n",
    "            ]\n",
    "\n",
    "            # Store lengths and angles as lists for correct display\n",
    "            formatted_cell_lengths = [f'{length}' for length in scaled_cell_lengths]\n",
    "            formatted_cell_angles = [f'{angle:.3f}' for angle in cell_angles]\n",
    "\n",
    "            # Calculate the number of atoms in the unit cell\n",
    "            num_atoms_in_unit_cell = num_atoms // (SUPERCELL_SIZE * SUPERCELL_SIZE)\n",
    "\n",
    "            # Create an Atoms object for the unit cell\n",
    "            unit_cell_atoms = atoms[:num_atoms_in_unit_cell]\n",
    "\n",
    "            # Get the chemical formula of the unit cell\n",
    "            formula = unit_cell_atoms.get_chemical_formula()\n",
    "\n",
    "            # Append data to data_list\n",
    "            data_list.append({\n",
    "                'Cluster': CLUSTER_NUM,\n",
    "                'Material': material_label,\n",
    "                'Full Lattice Formula': full_lattice_formula,\n",
    "                'Chemical Formula': formula,\n",
    "                'No. atoms per cell': num_atoms_in_unit_cell,\n",
    "                'Unitcell Lengths': formatted_cell_lengths,\n",
    "                'Cell Angles': formatted_cell_angles,\n",
    "                'Plot': plot_filepath  # Ensure that 'Plot' is the last column\n",
    "            })\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Structure file for material label {material_label} not found at {file_path}.\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing material {material_label}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return data_list\n",
    "\n",
    "# Load the formula mapping from the reduced_sublattice_structure.txt file\n",
    "formula_mapping = parse_reduced_sublattice_structure('reduced_sublattice_structure.txt')\n",
    "\n",
    "# List to store data for the table\n",
    "data_list = []\n",
    "\n",
    "# Process the specified cluster\n",
    "cluster_data = process_nodes_in_cluster(CLUSTER_NUM, hdb, cluster_labels, embedding_labels, formula_mapping)\n",
    "if cluster_data is not None:\n",
    "    data_list.extend(cluster_data)\n",
    "else:\n",
    "    print(f\"No data collected for cluster {CLUSTER_NUM}.\")\n",
    "\n",
    "# Define the column order, ensuring 'Plot' is the last column\n",
    "column_order = [\n",
    "    'Cluster',\n",
    "    'Material',\n",
    "    'Full Lattice Formula', \n",
    "    'Chemical Formula',\n",
    "    'No. atoms per cell',\n",
    "    'Unitcell Lengths',\n",
    "    'Cell Angles',\n",
    "    'Plot' \n",
    "]\n",
    "\n",
    "# Create a DataFrame from the collected data with the specified column order\n",
    "df = pd.DataFrame(data_list, columns=column_order)\n",
    "\n",
    "# Function to convert image paths to HTML tags for display\n",
    "def path_to_image_html(path):\n",
    "    return '<img src=\"{}\" height=\"200\">'.format(path)  # Set consistent height\n",
    "\n",
    "# Display the DataFrame with images in the 'Plot' column, suppressing the index column\n",
    "HTML(df.to_html(index=False, escape=False, formatters={'Plot': path_to_image_html}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Provides the properties of and visualises the core node of every cluster\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from ase.io import read\n",
    "from torch_geometric.data import Data\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Define the tolerance delta for nearest neighbors\n",
    "delta = 0.1  # Adjust as needed\n",
    "\n",
    "# Global parameter for the supercell size\n",
    "SUPERCELL_SIZE = 3\n",
    "\n",
    "# Define the folder to save plots\n",
    "plot_folder = 'Core Node Plots'\n",
    "\n",
    "# Ensure the plot folder exists\n",
    "if not os.path.exists(plot_folder):\n",
    "    os.makedirs(plot_folder)\n",
    "else:\n",
    "    # Delete all files in the directory\n",
    "    for filename in os.listdir(plot_folder):\n",
    "        file_path = os.path.join(plot_folder, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)  # Remove the file\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)  # Remove the directory\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {file_path}. Reason: {e}\")\n",
    "\n",
    "# Function to compute edge_index based on positions and delta, considering PBCs\n",
    "def compute_edge_index(atoms, delta):\n",
    "    positions = atoms.get_positions()\n",
    "    num_atoms = len(positions)\n",
    "\n",
    "    # Compute distance matrix considering PBCs\n",
    "    dist_matrix = atoms.get_all_distances(mic=True)\n",
    "    edge_index = set()  # Use a set to avoid duplicate edges\n",
    "\n",
    "    # For each node, find its nearest neighbors and next-nearest neighbors\n",
    "    for i in range(num_atoms):\n",
    "        # Exclude self-distance by setting diagonal to infinity\n",
    "        dist_matrix[i, i] = np.inf\n",
    "\n",
    "        # Get distances and sort them along with indices\n",
    "        distances = dist_matrix[i]\n",
    "        sorted_indices = np.argsort(distances)\n",
    "        sorted_distances = distances[sorted_indices]\n",
    "\n",
    "        # Nearest neighbor distance\n",
    "        d1 = sorted_distances[0]\n",
    "        nn_cutoff = d1 + delta\n",
    "\n",
    "        # Indices of nearest neighbors within d1 + delta\n",
    "        nn_indices = sorted_indices[sorted_distances <= nn_cutoff]\n",
    "\n",
    "        # Add edges from nearest neighbors to i\n",
    "        for j in nn_indices:\n",
    "            edge_index.add((j, i))\n",
    "\n",
    "        # Exclude nearest neighbors from consideration for next-nearest neighbors\n",
    "        remaining_indices = sorted_indices[sorted_distances > nn_cutoff]\n",
    "        remaining_distances = sorted_distances[sorted_distances > nn_cutoff]\n",
    "\n",
    "        if len(remaining_distances) > 0:\n",
    "            # Next-nearest neighbor distance\n",
    "            d2 = remaining_distances[0]\n",
    "            nnn_cutoff = d2 + delta\n",
    "\n",
    "            # Indices of next-nearest neighbors within d2 + delta\n",
    "            nnn_indices = remaining_indices[remaining_distances <= nnn_cutoff]\n",
    "\n",
    "            # Add edges from next-nearest neighbors to i\n",
    "            for j in nnn_indices:\n",
    "                edge_index.add((j, i))\n",
    "\n",
    "    # Convert edge_index to a tensor\n",
    "    if len(edge_index) > 0:\n",
    "        edge_index = torch.tensor(list(edge_index), dtype=torch.long).t().contiguous()\n",
    "    else:\n",
    "        # Handle graphs with no edges\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "    return edge_index\n",
    "\n",
    "# Function to compute edge attributes based on positions and edge_index, considering PBCs\n",
    "def compute_edge_attr(atoms, edge_index):\n",
    "    row, col = edge_index\n",
    "    positions = atoms.get_positions()\n",
    "    cell = atoms.get_cell()\n",
    "    scaled_positions = atoms.get_scaled_positions()\n",
    "\n",
    "    # Compute displacement vectors considering PBCs\n",
    "    delta_scaled = scaled_positions[row.numpy()] - scaled_positions[col.numpy()]\n",
    "    delta_scaled -= np.round(delta_scaled)\n",
    "    displacement_vectors = delta_scaled @ cell\n",
    "    edge_distances = np.linalg.norm(displacement_vectors, axis=1)\n",
    "    edge_attr = torch.tensor(edge_distances, dtype=torch.float).unsqueeze(1)\n",
    "    return edge_attr\n",
    "\n",
    "# Function to create a PyTorch Geometric Data object from an atomic structure\n",
    "def create_graph_from_structure(atoms, delta):\n",
    "    # Compute edge_index\n",
    "    edge_index = compute_edge_index(atoms, delta)\n",
    "\n",
    "    # Compute edge attributes\n",
    "    edge_attr = compute_edge_attr(atoms, edge_index)\n",
    "\n",
    "    # No node features\n",
    "    num_nodes = len(atoms)\n",
    "    node_features = torch.empty((num_nodes, 0), dtype=torch.float)\n",
    "\n",
    "    # Create a PyTorch Geometric Data object\n",
    "    graph = Data(\n",
    "        x=node_features,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "    )\n",
    "\n",
    "    return graph\n",
    "\n",
    "# Function to plot the graph with consistent y-size\n",
    "def plot_graph(atoms, graph, margin=0.1, tolerance=0.1, y_size=6, filename=None):\n",
    "    \"\"\"\n",
    "    Plots the graph of an atomic structure with consistent y-direction size.\n",
    "\n",
    "    Parameters:\n",
    "    - atoms: ASE Atoms object representing the structure.\n",
    "    - graph: PyTorch Geometric Data object for the graph.\n",
    "    - margin: Margin added to the plot boundaries as a fraction of plot size.\n",
    "    - tolerance: Tolerance for grouping atoms into layers by z-coordinate.\n",
    "    - y_size: Desired size of the plot in the y-direction (in inches).\n",
    "    - filename: If provided, saves the plot to this file instead of showing it.\n",
    "    \"\"\"\n",
    "    positions = atoms.get_positions()\n",
    "    cell = atoms.get_cell()\n",
    "\n",
    "    # Extract x, y, and z positions\n",
    "    x = positions[:, 0]\n",
    "    y = positions[:, 1]\n",
    "    z = positions[:, 2]\n",
    "\n",
    "    # Group z positions into layers using the tolerance\n",
    "    z_grouped = np.round(z / tolerance) * tolerance\n",
    "\n",
    "    # Get unique z positions and assign an index to each layer\n",
    "    z_unique, indices = np.unique(z_grouped, return_inverse=True)\n",
    "    N_layers = len(z_unique)\n",
    "\n",
    "    # Define a colormap with a number of colors equal to the number of layers\n",
    "    cmap = plt.cm.get_cmap('viridis', N_layers)\n",
    "\n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Shift supercell boundary by 0.5c\n",
    "    c_shift = 0.5 * cell[2]  # Half the c vector\n",
    "    corner_positions = np.array([\n",
    "        [0, 0, 0],\n",
    "        cell[0],\n",
    "        cell[0] + cell[1],\n",
    "        cell[1],\n",
    "        [0, 0, 0]\n",
    "    ]) + c_shift  # Add the c-shift vector to each vertex\n",
    "\n",
    "    # Recalculate plot limits based on the shifted boundary\n",
    "    x_min, x_max = corner_positions[:, 0].min(), corner_positions[:, 0].max()\n",
    "    y_min, y_max = corner_positions[:, 1].min(), corner_positions[:, 1].max()\n",
    "\n",
    "    # Add margin to the limits\n",
    "    x_margin = (x_max - x_min) * margin\n",
    "    y_margin = (y_max - y_min) * margin\n",
    "\n",
    "    x_min -= x_margin\n",
    "    x_max += x_margin\n",
    "    y_min -= y_margin\n",
    "    y_max += y_margin\n",
    "\n",
    "    # Dynamically adjust the figure size to maintain consistent y-direction size\n",
    "    y_range = y_max - y_min\n",
    "    x_range = x_max - x_min\n",
    "    aspect_ratio = x_range / y_range\n",
    "\n",
    "    fig.set_size_inches(y_size * aspect_ratio, y_size)  # Scale x-size proportionally\n",
    "\n",
    "    # Plot the supercell boundary\n",
    "    ax.plot(corner_positions[:, 0], corner_positions[:, 1], 'k--', linewidth=1)\n",
    "\n",
    "    # Plot the nodes with colors based on z position\n",
    "    ax.scatter(x, y, c=indices, s=50, cmap=cmap, zorder=2)\n",
    "\n",
    "    # Prepare edge lines\n",
    "    lines = []\n",
    "    for idx in range(graph.edge_index.shape[1]):\n",
    "        i = graph.edge_index[0, idx].item()\n",
    "        j = graph.edge_index[1, idx].item()\n",
    "\n",
    "        pos_i = positions[i]\n",
    "        pos_j = positions[j]\n",
    "\n",
    "        # Compute delta_scaled considering PBCs\n",
    "        delta_scaled = atoms.get_scaled_positions()[j] - atoms.get_scaled_positions()[i]\n",
    "        delta_scaled -= np.round(delta_scaled)\n",
    "\n",
    "        # Adjusted position of j for plotting\n",
    "        delta = delta_scaled @ cell\n",
    "        pos_j_plot = pos_i + delta\n",
    "\n",
    "        # Add the line segment\n",
    "        lines.append([pos_i[:2], pos_j_plot[:2]])\n",
    "\n",
    "    # Create a LineCollection from the lines\n",
    "    lc = LineCollection(lines, colors='gray', linewidths=1, zorder=1)\n",
    "    ax.add_collection(lc)\n",
    "\n",
    "    # Set limits with new boundaries\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "\n",
    "    # Set aspect ratio to equal\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    # Remove axes and adjust layout\n",
    "    ax.axis('off')\n",
    "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "\n",
    "    # Save the plot to a file or show it\n",
    "    if filename:\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "# Function to process the core node of a cluster and collect data\n",
    "def process_core_node_of_cluster(CLUSTER_NUM, hdb, cluster_labels, embedding_labels):\n",
    "    # Find indices of points in the given cluster\n",
    "    cluster_indices = np.where(cluster_labels == CLUSTER_NUM)[0]\n",
    "\n",
    "    if len(cluster_indices) == 0:\n",
    "        print(f\"No points found in cluster {CLUSTER_NUM}.\")\n",
    "        return None\n",
    "\n",
    "    # Get the probabilities for these points\n",
    "    cluster_probabilities = hdb.probabilities_[cluster_indices]\n",
    "\n",
    "    # Find the index with the highest probability (most representative core point)\n",
    "    max_prob_index_in_cluster = cluster_indices[np.argmax(cluster_probabilities)]\n",
    "\n",
    "    # Get the material label\n",
    "    material_label = int(embedding_labels[max_prob_index_in_cluster])\n",
    "\n",
    "    # Build the file path\n",
    "    file_path = f'supercells_flatband_rotated_shifted_aligned_3x3/supercell_2dm-{material_label}.xyz'\n",
    "\n",
    "    # Process the structure\n",
    "    try:\n",
    "        atoms = read(file_path)\n",
    "        atoms.pbc = [True, True, False]  # Ensure PBCs are enabled\n",
    "\n",
    "        # Create graph with PBCs\n",
    "        graph = create_graph_from_structure(atoms, delta)\n",
    "\n",
    "        # Save the plot to a file in the plot_folder\n",
    "        plot_filename = f'graph_cluster_{CLUSTER_NUM}_material_{material_label}.png'\n",
    "        plot_filepath = os.path.join(plot_folder, plot_filename)\n",
    "        plot_graph(atoms, graph, margin=0.2, tolerance=0.1, y_size=6, filename=plot_filepath)\n",
    "        print(f\"Graph for material label {material_label} from cluster {CLUSTER_NUM} saved as {plot_filepath}.\")\n",
    "\n",
    "        # Extract additional properties\n",
    "        num_atoms = len(atoms)  # Number of atoms\n",
    "        cell_lengths = atoms.cell.diagonal()  # Get lengths of the cell\n",
    "        cell_angles = atoms.cell.cellpar()[3:6]  # Get angles in degrees\n",
    "\n",
    "        # Scale the first two cell lengths by dividing by SUPERCELL_SIZE\n",
    "        scaled_cell_lengths = [\n",
    "            f'{cell_lengths[0] / SUPERCELL_SIZE:.3f}',\n",
    "            f'{cell_lengths[1] / SUPERCELL_SIZE:.3f}',\n",
    "            f'{cell_lengths[2]:.3f}'\n",
    "        ]\n",
    "\n",
    "        # Store lengths and angles as lists for correct display\n",
    "        formatted_cell_lengths = [f'{length}' for length in scaled_cell_lengths]\n",
    "        formatted_cell_angles = [f'{angle:.3f}' for angle in cell_angles]\n",
    "\n",
    "        # Calculate the number of atoms in the unit cell\n",
    "        num_atoms_in_unit_cell = num_atoms // (SUPERCELL_SIZE * SUPERCELL_SIZE)\n",
    "\n",
    "        # Create an Atoms object for the unit cell\n",
    "        unit_cell_atoms = atoms[:num_atoms_in_unit_cell]\n",
    "\n",
    "        # Get the chemical formula of the unit cell\n",
    "        formula = unit_cell_atoms.get_chemical_formula()\n",
    "\n",
    "        # Return the data for the table, including additional properties\n",
    "        return {\n",
    "            'Cluster': CLUSTER_NUM,\n",
    "            'Material': material_label,\n",
    "            'Chemical Formula': formula,\n",
    "            'No. atoms per cell': num_atoms_in_unit_cell,\n",
    "            'Unitcell Lengths': formatted_cell_lengths,\n",
    "            'Cell Angles': formatted_cell_angles,\n",
    "            'Plot': plot_filepath  # Ensure that 'Plot' is the last column\n",
    "        }\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Structure file for material label {material_label} not found at {file_path}.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the structure: {e}\")\n",
    "        return None\n",
    "\n",
    "# Ensure that embedding_labels, cluster_labels, and hdb are available\n",
    "# If not already loaded, load them using your existing function\n",
    "# embedding_labels, cluster_labels = load_clusters_with_labels('clustered_labels.npy')\n",
    "\n",
    "# Get all unique cluster numbers (excluding noise points labeled as -1)\n",
    "cluster_numbers = np.unique(cluster_labels)\n",
    "cluster_numbers = cluster_numbers[cluster_numbers != -1]  # Exclude noise\n",
    "\n",
    "# List to store data for the table\n",
    "data_list = []\n",
    "\n",
    "# Loop over each cluster and collect data\n",
    "for CLUSTER_NUM in cluster_numbers:\n",
    "    result = process_core_node_of_cluster(CLUSTER_NUM, hdb, cluster_labels, embedding_labels)\n",
    "    if result is not None:\n",
    "        data_list.append(result)\n",
    "\n",
    "# Define the column order, ensuring 'Plot' is the last column and rearranged as per your request\n",
    "column_order = [\n",
    "    'Cluster',\n",
    "    'Material',\n",
    "    'Chemical Formula',\n",
    "    'No. atoms per cell',  # Updated column name\n",
    "    'Unitcell Lengths',\n",
    "    'Cell Angles',\n",
    "    'Plot'  # Ensure 'Plot' is the last column\n",
    "]\n",
    "\n",
    "# Create a DataFrame from the collected data with the specified column order\n",
    "df = pd.DataFrame(data_list, columns=column_order)\n",
    "\n",
    "# Function to convert image paths to HTML tags for display\n",
    "def path_to_image_html(path):\n",
    "    return '<img src=\"{}\" height=\"200\">'.format(path)  # Set consistent height\n",
    "\n",
    "# Display the DataFrame with images in the 'Plot' column, suppressing the index column\n",
    "HTML(df.to_html(index=False, escape=False, formatters={'Plot': path_to_image_html}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
