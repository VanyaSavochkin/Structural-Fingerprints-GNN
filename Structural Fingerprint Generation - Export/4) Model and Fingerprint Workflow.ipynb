{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training and Fingerprint Generation Workflow: takes the perturbed graph lists generated in 3) to train a GNN encoder \n",
    "# then generates a set of final embeddings using the unperturbed graph list and the trained model.\n",
    "# Naming: saves the weights of the trained model as '{MODEL_NAME}.pth', the final embeddings as 'embeddings_{MODEL_NAME}.pt'\n",
    "# and the final embedding labels as 'labels_{MODEL_NAME}.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables:\n",
    "\n",
    "MODEL_NAME = 'Test'\n",
    "SUPERCELL_SIZE = 3 # Size of supercell used in graph list 1 in (NxN) unit cells. Same as SUPERCELL_SIZE_1 in 3)\n",
    "BATCH_SIZE = 48 # Number of materials ran through the network at a time\n",
    "NUM_EPOCHS = 1000 # How many times each trainer runs through the training data\n",
    "LEARNING_RATE = 1e-4 # Adjusts how sensitive the network is when changing the weights\n",
    "EMBEDDING_DIMENSION = 192 # How many dimensions the final vector (structural fingerprint)has\n",
    "HIDDEN_DIMENSION = 192 # How many nodes in the hidden layer of the neural network\n",
    "LAYER_NUMBER = 6\n",
    "DROPOUT_RATE = 0.1 # set the rate of dropout during training\n",
    "WEIGHT_DECAY = 1e-5 # sets the level of weight decay during training\n",
    "TEMPERATURE = 0.07 # Quanitifies how much negative samples are 'pushed' from positive samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph list loader: loads the graph lists generated in 3)\n",
    "\n",
    "import torch\n",
    "\n",
    "# Folder location of graph lists\n",
    "INPUT_FOLDER = 'Graphs/2DMatpedia Sublattices'\n",
    "\n",
    "# Load unperturbed graph list 1\n",
    "graph_list_unperturbed_1 = torch.load(f\"{INPUT_FOLDER}\\graph_list_unperturbed_1.pt\")\n",
    "\n",
    "# Load the first perturbed graph list\n",
    "graph_list_set_1 = torch.load(f\"{INPUT_FOLDER}\\graph_list_set_1.pt\")\n",
    "\n",
    "# Load the second perturbed graph list\n",
    "graph_list_set_2 = torch.load(f\"{INPUT_FOLDER}\\graph_list_set_2.pt\")\n",
    "\n",
    "print(\"Successfully loaded all three graph lists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN Encoder: takes a graph and carries out hyperedge message passing using a custom message passing layer \n",
    "# and generates an embedding or fingerprint using global attention pooling.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing, GlobalAttention\n",
    "from torch_geometric.utils import add_self_loops\n",
    "from torch_geometric.nn.norm import LayerNorm\n",
    "\n",
    "# 1. Class that defines the hyperedge message passing layer\n",
    "class HyperedgeGNNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, edge_dim, hyperedge_dim, aggr='add'):\n",
    "        super(HyperedgeGNNConv, self).__init__(aggr=aggr)\n",
    "\n",
    "        self.lin_node = nn.Linear(in_channels, out_channels)\n",
    "        self.lin_edge = nn.Linear(in_channels + edge_dim, out_channels)\n",
    "        self.lin_hyperedge = nn.Linear(in_channels + hyperedge_dim, out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, hyperedge_index, hyperedge_attr):\n",
    "        # ---- Node & Edge transforms ----\n",
    "        x = self.lin_node(x)\n",
    "        edge_messages = self.propagate(edge_index, x=x, edge_attr=edge_attr, mode='edge')\n",
    "        \n",
    "        # ---- Build hyperedge edges (i->j, k->j), but REMOVE (j->j) ----\n",
    "        # hyperedge_index shape = [3, num_hyperedges], representing (i, j, k) for each hyperedge\n",
    "        num_hyperedges = hyperedge_index.size(1)\n",
    "\n",
    "        # We only want i->j and k->j:\n",
    "        sender_indices = torch.cat([hyperedge_index[0], hyperedge_index[2]])  # i, k\n",
    "        receiver_indices = hyperedge_index[1].repeat(2)  # j repeated 2 times\n",
    "\n",
    "        # Construct edge_index for hyperedges\n",
    "        hyperedge_edge_index = torch.stack([sender_indices, receiver_indices], dim=0)\n",
    "\n",
    "        # Repeat hyperedge attributes for i->j and k->j (2 edges), not 3\n",
    "        hyperedge_edge_attr = hyperedge_attr.repeat(2, 1)\n",
    "\n",
    "        # Propagate hyperedge messages\n",
    "        hyperedge_messages = self.propagate(\n",
    "            hyperedge_edge_index, x=x, edge_attr=hyperedge_edge_attr, mode='hyperedge'\n",
    "        )\n",
    "\n",
    "        # ---- Combine everything ----\n",
    "        out = x + edge_messages + hyperedge_messages\n",
    "        return self.relu(out)\n",
    "\n",
    "    def message(self, x_j, edge_attr, mode):\n",
    "        if mode == 'edge':\n",
    "            msg_input = torch.cat([x_j, edge_attr], dim=-1)\n",
    "            msg = self.lin_edge(msg_input)\n",
    "        elif mode == 'hyperedge':\n",
    "            msg_input = torch.cat([x_j, edge_attr], dim=-1)\n",
    "            msg = self.lin_hyperedge(msg_input)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode. Use 'edge' or 'hyperedge'.\")\n",
    "        return msg\n",
    "\n",
    "# 2. Class that defines the shape of the GNN encoder using the hyperedge message passing layer\n",
    "class GNNWithHyperedges(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_node_features,\n",
    "        num_edge_features,\n",
    "        num_hyperedge_features,\n",
    "        hidden_dim=128,\n",
    "        embedding_dim=64,\n",
    "        num_layers=4,\n",
    "        dropout_rate=0.1,\n",
    "    ):\n",
    "        super(GNNWithHyperedges, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.node_embedding = nn.Linear(num_node_features, hidden_dim)\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            conv = HyperedgeGNNConv(\n",
    "                in_channels=hidden_dim,\n",
    "                out_channels=hidden_dim,\n",
    "                edge_dim=num_edge_features,\n",
    "                hyperedge_dim=num_hyperedge_features,\n",
    "                aggr='mean',\n",
    "            )\n",
    "            self.convs.append(conv)\n",
    "            self.norms.append(LayerNorm(hidden_dim))\n",
    "        \n",
    "        # Global Attention Pooling\n",
    "        self.attention_pool = GlobalAttention(gate_nn=nn.Linear(hidden_dim, 1))\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout_rate),\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        edge_index = data.edge_index\n",
    "        edge_attr = data.edge_attr\n",
    "        hyperedge_index = data.hyperedge_index\n",
    "        hyperedge_attr = data.hyperedge_attr\n",
    "\n",
    "        batch = data.batch if hasattr(data, 'batch') else torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n",
    "\n",
    "        x = self.node_embedding(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "\n",
    "        for conv, norm in zip(self.convs, self.norms):\n",
    "            x_residual = x\n",
    "            x = conv(x, edge_index, edge_attr, hyperedge_index, hyperedge_attr)\n",
    "            x = norm(x)\n",
    "            x = F.relu(x) + x_residual\n",
    "            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "\n",
    "        # Apply Global Attention pooling instead of mean pooling\n",
    "        x = self.attention_pool(x, batch)\n",
    "\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function: uses the InfoNCE loss function\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def info_nce_loss(z1, z2, temperature=0.07):\n",
    "    \"\"\"\n",
    "    Computes the InfoNCE (NT-Xent) loss for two batches of embeddings z1, z2.\n",
    "    Each batch has size N, so total 2N embeddings.\n",
    "    \n",
    "    Args:\n",
    "        z1: Tensor of shape (N, d), embeddings for batch 1\n",
    "        z2: Tensor of shape (N, d), embeddings for batch 2\n",
    "        temperature: Softmax temperature (float)\n",
    "    Returns:\n",
    "        A scalar tensor representing the contrastive loss.\n",
    "    \"\"\"\n",
    "    z1 = F.normalize(z1, dim=1)\n",
    "    z2 = F.normalize(z2, dim=1)\n",
    "    \n",
    "    z = torch.cat([z1, z2], dim=0) \n",
    "    \n",
    "    sim_matrix = torch.matmul(z, z.t())\n",
    "\n",
    "    sim_matrix = sim_matrix / temperature\n",
    "    \n",
    "    N = z1.shape[0]\n",
    "\n",
    "    pos_indices = torch.arange(N, 2*N)\n",
    "    neg_indices = torch.arange(0, N)\n",
    "\n",
    "    pos_index = torch.cat([pos_indices, neg_indices], dim=0).to(z.device)\n",
    "\n",
    "    labels = pos_index\n",
    "\n",
    "    mask = torch.eye(2*N, dtype=torch.bool, device=z.device)\n",
    "    sim_matrix = sim_matrix.masked_fill(mask, float('-inf'))\n",
    "    \n",
    "    loss = F.cross_entropy(sim_matrix, labels)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader: splits the graph lists into training and validation sets with an 80/20 split using a fixed seed for reproducibility\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Calculates training and validation set sizes\n",
    "NUM_TRAINING = int(len(graph_list_set_1) * 0.8)  # 80% for training\n",
    "NUM_VALIDATE = len(graph_list_set_1) - NUM_TRAINING  # Remaining for validation\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "fixed_seed = 42\n",
    "np.random.seed(fixed_seed)\n",
    "indices = np.random.permutation(len(graph_list_set_1))\n",
    "\n",
    "# Shuffle the graph lists\n",
    "shuffled_graph_list_set_1 = [graph_list_set_1[i] for i in indices]\n",
    "shuffled_graph_list_set_2 = [graph_list_set_2[i] for i in indices]\n",
    "shuffled_graph_list_set_3 = [graph_list_unperturbed_1[i] for i in indices]\n",
    "\n",
    "# Split the datasets into training and validation sets\n",
    "train_graph_list_set_1 = shuffled_graph_list_set_1[:NUM_TRAINING]\n",
    "val_graph_list_set_1 = shuffled_graph_list_set_1[NUM_TRAINING:]\n",
    "\n",
    "train_graph_list_set_2 = shuffled_graph_list_set_2[:NUM_TRAINING]\n",
    "val_graph_list_set_2 = shuffled_graph_list_set_2[NUM_TRAINING:]\n",
    "\n",
    "# Create DataLoaders for training and validation sets\n",
    "train_loader_set_1 = DataLoader(train_graph_list_set_1, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader_set_1 = DataLoader(val_graph_list_set_1, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "train_loader_set_2 = DataLoader(train_graph_list_set_2, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader_set_2 = DataLoader(val_graph_list_set_2, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"Data loaders created for training and validation sets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer: initialises the encoder with random weights then trains using the training dataset. Prints loss for valdiation and training\n",
    "# and saves the model with the best validation loss\n",
    "# Note: you can stop this code cell from running at any point and it will still save the current best model\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Directories\n",
    "os.makedirs(\"Models\", exist_ok=True)\n",
    "os.makedirs(\"Plots and Visualisations/Training Plots\", exist_ok=True)\n",
    "\n",
    "# Print Training Setup\n",
    "print(f\"\"\"\n",
    "-----------training starting---------------\n",
    "BATCH_SIZE: {BATCH_SIZE}\n",
    "NUM_EPOCHS: {NUM_EPOCHS}\n",
    "Learning Rate: {LEARNING_RATE}\n",
    "Hidden Dimension: {HIDDEN_DIMENSION}\n",
    "Embedding Dimension: {EMBEDDING_DIMENSION}\n",
    "Temperature (InfoNCE): {TEMPERATURE}\n",
    "\"\"\")\n",
    "\n",
    "# 1. Initialise the Model\n",
    "model = GNNWithHyperedges(\n",
    "    num_node_features=graph_list_set_1[0].x.size(1),  # Adjust based on node features\n",
    "    num_edge_features=graph_list_set_1[0].edge_attr.size(1),  # Adjust based on edge features\n",
    "    num_hyperedge_features=graph_list_set_1[0].hyperedge_attr.size(1),  # Adjust based on hyperedge features\n",
    "    hidden_dim=HIDDEN_DIMENSION,\n",
    "    embedding_dim=EMBEDDING_DIMENSION,\n",
    "    num_layers=LAYER_NUMBER,  # Customize as needed\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# 2. InfoNCE (NT-Xent) Loss Function\n",
    "def info_nce_loss(z1, z2, temperature=0.07):\n",
    "    \"\"\"\n",
    "    Computes the InfoNCE (NT-Xent) loss for two batches of embeddings z1, z2.\n",
    "    Each batch has size N, so we get 2N embeddings total.\n",
    "    \"\"\"\n",
    "    # Normalize embeddings\n",
    "    z1 = F.normalize(z1, dim=1)\n",
    "    z2 = F.normalize(z2, dim=1)\n",
    "\n",
    "    N = z1.size(0)\n",
    "    # Concatenate along batch dimension: 2N x d\n",
    "    z = torch.cat([z1, z2], dim=0)\n",
    "\n",
    "    # Pairwise similarity (2N x 2N)\n",
    "    sim_matrix = torch.matmul(z, z.t()) / temperature\n",
    "\n",
    "    # Labels: each sample in [0..N-1] has positive at index i+N\n",
    "    # in [N..2N-1], positive at index i-N\n",
    "    pos_indices = torch.arange(N, 2*N)\n",
    "    neg_indices = torch.arange(0, N)\n",
    "    pos_index = torch.cat([pos_indices, neg_indices], dim=0).to(device)\n",
    "    labels = pos_index\n",
    "\n",
    "    # Mask out self-similarity\n",
    "    mask = torch.eye(2*N, dtype=torch.bool, device=device)\n",
    "    sim_matrix = sim_matrix.masked_fill(mask, float('-inf'))\n",
    "\n",
    "    # Cross-entropy over rows\n",
    "    loss = F.cross_entropy(sim_matrix, labels)\n",
    "    return loss\n",
    "\n",
    "# 3. Compute Loss Wrapper\n",
    "def compute_loss(model, data1, data2):\n",
    "    data1, data2 = data1.to(device), data2.to(device)\n",
    "    embedding1 = model(data1)\n",
    "    embedding2 = model(data2)\n",
    "    return info_nce_loss(embedding1, embedding2, temperature=TEMPERATURE)\n",
    "\n",
    "# 4. Training for One Epoch\n",
    "def train_one_epoch(model, loader_set_1, loader_set_2, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for data1, data2 in zip(loader_set_1, loader_set_2):\n",
    "        optimizer.zero_grad()\n",
    "        loss = compute_loss(model, data1, data2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(loader_set_1)\n",
    "    return avg_train_loss\n",
    "\n",
    "# 5. Validation \n",
    "def validate_model(model, loader_set_1, loader_set_2):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data1, data2 in zip(loader_set_1, loader_set_2):\n",
    "            loss = compute_loss(model, data1, data2)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_loss / len(loader_set_1)\n",
    "    return avg_val_loss\n",
    "\n",
    "# 6. Full Training Loop\n",
    "def train_model(model, train_loader_1, train_loader_2, val_loader_1, val_loader_2, \n",
    "                optimizer, num_epochs):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')  # Initialize best validation loss\n",
    "\n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            # Training\n",
    "            avg_train_loss = train_one_epoch(model, train_loader_1, train_loader_2, optimizer)\n",
    "            train_losses.append(avg_train_loss)\n",
    "\n",
    "            # Validation\n",
    "            avg_val_loss = validate_model(model, val_loader_1, val_loader_2)\n",
    "            val_losses.append(avg_val_loss)\n",
    "\n",
    "            # Check best validation loss\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                torch.save(model.state_dict(), f'Models/{MODEL_NAME}.pth')\n",
    "                print(f\"New best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "                  f\"Training Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted by user. Best model saved so far.\")\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# 7. Plotting \n",
    "def plot_losses(train_losses, val_losses):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss', marker='o')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss', marker='o')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"Plots and Visualisations/Training Plots/{MODEL_NAME}.png\")\n",
    "    plt.show()\n",
    "\n",
    "# 8. Run Training\n",
    "train_losses, val_losses = train_model(\n",
    "    model, \n",
    "    train_loader_set_1, \n",
    "    train_loader_set_2, \n",
    "    val_loader_set_1, \n",
    "    val_loader_set_2, \n",
    "    optimizer, \n",
    "    NUM_EPOCHS\n",
    ")\n",
    "\n",
    "plot_losses(train_losses, val_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Embedding generator: takes the trained model and generates a list of final embeddings using the unperturbed graph list\n",
    "# Saves the embeddings along with their corresponding labels\n",
    "\n",
    "os.makedirs('Embeddings', exist_ok=True)\n",
    "os.makedirs('Labels', exist_ok=True)\n",
    "\n",
    "# 1. Function that runs each graph through the model extracting the final embedding and corresponding graph label, returns them both\n",
    "def test_model(model, loader):\n",
    "    model.eval()\n",
    "    embeddings_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(loader):\n",
    "            data = data.to(device)\n",
    "            embeddings = model(data)\n",
    "            embeddings_list.append(embeddings.cpu())\n",
    "            labels_list.extend(data.label)\n",
    "\n",
    "    # Concatenate embeddings into a single tensor\n",
    "    all_embeddings = torch.cat(embeddings_list, dim=0)\n",
    "    \n",
    "    # Labels remain a list of strings\n",
    "    return all_embeddings, labels_list\n",
    "\n",
    "# 2. loads the needed data, generates the graph with labels and saves them\n",
    "\n",
    "test_loader = DataLoader(graph_list_unperturbed_1, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Call the test function to generate embeddings and labels for the entire test set\n",
    "output_graph_embeddings, output_graph_labels = test_model(model, test_loader)\n",
    "\n",
    "# Print the shapes of the final embeddings and labels\n",
    "print(f\"Generated embeddings shape: {output_graph_embeddings.shape}\")\n",
    "print(f\"Sample labels: {output_graph_labels[:5]}\")\n",
    "\n",
    "embeddings_path = f'Embeddings/embeddings_{MODEL_NAME}.pt'\n",
    "labels_path = f'Labels/labels_{MODEL_NAME}.pt'\n",
    "\n",
    "torch.save(output_graph_embeddings, embeddings_path)\n",
    "\n",
    "# Save the validation graph embeddings\n",
    "torch.save(output_graph_labels, labels_path)\n",
    "\n",
    "# Print confirmation message\n",
    "print(f\"embeddings saved to {embeddings_path}\")\n",
    "print(f\"labels saved to {labels_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
